{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCq8kJ42ghDl"
   },
   "source": [
    "### Sample Code for SVMs via sub-gradient descent and quadratic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vNN6eZW5ghDm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import csv\n",
    "import cvxopt\n",
    "\n",
    "def create_download_file(fname, preds):\n",
    "    \"\"\"Create file with predictions written as a csv file\n",
    "    \"\"\"\n",
    "    ofile  = open(fname, \"w\")  \n",
    "    writer = csv.writer(\n",
    "        ofile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "    writer.writerow(['id', 'category'])\n",
    "    for i in range(preds.shape[0]):\n",
    "        writer.writerow([i, preds[i]])\n",
    "\n",
    "\n",
    "def plot_decision_countour(svm, X, y, grid_size=100):\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
    "                         np.linspace(y_min, y_max, grid_size),\n",
    "                         indexing='ij')\n",
    "    data = np.stack([xx, yy], axis=2).reshape(-1, 2)\n",
    "    pred = svm.predict(data).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, pred,\n",
    "                 cmap=cm.Paired,\n",
    "                 levels=[-0.001, 0.001],\n",
    "                 extend='both',\n",
    "                 alpha=0.8)\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "    plt.scatter(flatten(X[:,0][y==-1]),flatten(X[:,1][y==-1]),\n",
    "                  c=flatten(y)[y==-1],cmap=cm.Paired,marker='o')\n",
    "    plt.scatter(flatten(X[:,0][y==1]),flatten(X[:,1][y==1]),\n",
    "                  c=flatten(y)[y==1],cmap=cm.Paired,marker='+')\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_SVM(svm, num_samples=500,linear=False):\n",
    "    \"\"\"test svm\n",
    "    \"\"\"\n",
    "    np.random.seed(783923)\n",
    "\n",
    "    X = npr.random((num_samples, 2)) * 2 - 1\n",
    "    if linear:\n",
    "      y = 2 * (X.sum(axis=1) > 0) - 1.0\n",
    "    else: \n",
    "      y = 2 * ((X ** 2).sum(axis=1) - 0.5 > 0) - 1.0\n",
    "    svm.fit(X,y)\n",
    "    \n",
    "    plot_decision_countour(svm, X, y)\n",
    "\n",
    "    from datetime import datetime\n",
    "    np.random.seed(int(round(datetime.now().timestamp())))\n",
    "\n",
    "def compute_acc(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    size = len(y)\n",
    "    num_correct = (pred == y).sum()\n",
    "    acc = num_correct / size\n",
    "    print(\"{} out of {} correct, acc {:.3f}\".format(num_correct, size, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNDW9Ia2ghDq"
   },
   "outputs": [],
   "source": [
    "#@title Linear SVM class **(To be modified)**\n",
    "#@markdown This class contains a shell of an implementation of a linear SVM.\n",
    "#@markdown Double click to expand and see what needs to be implemented.\n",
    "\n",
    "class LinearSVM():\n",
    "    def __init__(self,C):\n",
    "        \"\"\"initialize the svm\n",
    "        \n",
    "        Args:\n",
    "            C: weight associated with the hinge loss term in the SVM loss\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        self.bias = None\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y,num_epochs=30,lr_sched=lambda t: 0.1/t):\n",
    "        \"\"\"Fit the model on the data\n",
    "        \n",
    "        Args:\n",
    "            X: [N x d] data matrix\n",
    "            y: [N, ] array of labels\n",
    "            num_epochs: number of passes over the training data we make\n",
    "            lr_sched: function determining how the learning rate decays across\n",
    "                      epochs\n",
    "        \n",
    "        Returns:\n",
    "            self, in case you want to build a pipeline\n",
    "        \"\"\"\n",
    "        assert np.ndim(X) == 2, 'data matrix X expected to be 2d'\n",
    "        assert np.ndim(y) == 1, 'labels expected to be 1d'\n",
    "        N, d = X.shape\n",
    "        assert N == y.shape[0], 'expect [N, d] data matrix and [N] labels'\n",
    "        self.w = np.zeros([d,1])\n",
    "        self.bias = 0\n",
    "        # TODO: implement a subgradient descent\n",
    "        y = y.reshape((N,1))\n",
    "        for n in range(N//num_epochs):\n",
    "          h = y*(np.dot(X,self.w)+self.bias)\n",
    "          a = np.random.randint(N - num_epochs)\n",
    "          for i in range(a,a+num_epochs):\n",
    "            if (h[i] < 1):\n",
    "              xy = (y[i]*X[i])\n",
    "              xy = xy.reshape(d,1)\n",
    "              self.w = self.w + lr_sched(num_epochs) * xy\n",
    "              self.bias = self.bias + lr_sched(num_epochs) * y[i]\n",
    "            else:\n",
    "              self.w = self.w\n",
    "              self.bias = self.bias\n",
    "        \n",
    "        print(\"training complete\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, binarize=True):\n",
    "        \"\"\"make a prediction and return either the confidence margin or label\n",
    "        \n",
    "        Args:\n",
    "            X: [N, d] array of data or [d,] single data point\n",
    "            binarize: if True, then return the label, else the confidence margin\n",
    "        \n",
    "        Returns:\n",
    "            Either confidence margin or predicted label\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise ValueError(\"go fit the data first\")\n",
    "        X = np.atleast_2d(X)\n",
    "        assert X.shape[1] == self.w.shape[0]\n",
    "        res = X.dot(self.w)\n",
    "        res = res.squeeze()+self.bias\n",
    "        if binarize:\n",
    "            return (res > 0).astype(np.int32) * 2 - 1\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"construct a fresh copy of myself\n",
    "        \"\"\"\n",
    "        return LinearSVM(self.lamb, self.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "M1cTXCKYghDv"
   },
   "outputs": [],
   "source": [
    "#@title Kernel class. \n",
    "#@markdown This class contains a base implementation of several common\n",
    "#@markdown kernels. If you want to implement custom kernels, you should do that\n",
    "#@markdown here.\n",
    "\n",
    "\n",
    "class Kernel(object):\n",
    "    \"\"\"\n",
    "    A class containing all kinds of kernels.\n",
    "    Note: the kernel should work for both input (Matrix, vector) and (vector, vector)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(x, y):\n",
    "            return np.dot(x, y)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        def f(x, y):\n",
    "            exponent = - (1/sigma**2) * np.linalg.norm((x-np.transpose(y)).transpose(), 2, 0) ** 2\n",
    "            return np.exp(exponent)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def _poly(dimension, offset):\n",
    "        def f(x, y):\n",
    "            return (offset + np.dot(x, y)) ** dimension\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def inhomogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def homogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        def f(x, y):\n",
    "            return np.tanh(kappa * np.dot(x, y) + c)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSlLz8W7ghDw"
   },
   "outputs": [],
   "source": [
    "#@title Kernel SVM class **(To be modified)**\n",
    "#@markdown In this class, you will fill in the missing functions to\n",
    "#@markdown complete the implementation of the Kernel SVM. We will set up the QP\n",
    "#@markdown optimization and plug it into cvxopt, a black-box convex optimization\n",
    "#@markdown library for Python.\n",
    "\n",
    "class KernelSVM(object):\n",
    "    def __init__(self, kernel, C):\n",
    "        \"\"\"\n",
    "        Build a SVM given kernel function and C\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kernel : function\n",
    "            a function takes input (Matrix, vector) or (vector, vector)\n",
    "        C : a scalar\n",
    "            balance term\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        self._kernel = kernel\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model given data X and ground truth label y\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        # Solve the QP problem to get the multipliers\n",
    "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
    "        # Get all the support vectors, support weights and bias\n",
    "        self._construct_predictor(X, y, lagrange_multipliers)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the label given data X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1D array\n",
    "            predicted label\n",
    "        \"\"\"\n",
    "        result = np.full(X.shape[0], self._bias) # note: intializing scores with b\n",
    "        for z_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            result += z_i * y_i * self._kernel(X, x_i) # the result is \\sum_i alpha_i*y_i*x_i+b\n",
    "        return np.sign(result)\n",
    "\n",
    "    def _kernel_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Get the kernel matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        K : 2D array\n",
    "            N x N kernel matrix, where K[i][j] = inner_product(phi(i), phi(j))\n",
    "        \"\"\"\n",
    "        K = self._kernel(X,np.transpose(X))\n",
    "        return K\n",
    "        pass\n",
    "\n",
    "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        \"\"\"\n",
    "        Given the data, label and the multipliers, extract the support vectors and calculate the bias\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "        lagrange_multipliers: 1D array\n",
    "            the solution of lagrange_multiplier\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        support_vector_indices = \\\n",
    "            lagrange_multipliers > 1e-5\n",
    "            \n",
    "        print(\"SV number: \", np.sum(support_vector_indices))\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        \"\"\"\n",
    "        Get the bias term\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        #N,d = support_vectors.shape\n",
    "        #bias = 0\n",
    "        #for i in range(N):\n",
    "        #  K = self._kernel(X, X[1])\n",
    "        #  for j in range(N):\n",
    "        #    bias = bias + support_multipliers[j] * support_vector_labels[j] * K[j]\n",
    "        #bias = (np.sum(y) - bias)/N\n",
    "        K = self._kernel_matrix(support_vectors)\n",
    "        weights = support_multipliers * support_vector_labels\n",
    "        y_hat = np.dot(weights,K)\n",
    "        npsum = np.sum(support_vector_indices)\n",
    "        bias = 1/npsum * np.sum((support_vector_labels - y_hat))\n",
    "        self._bias=bias\n",
    "        self._weights=support_multipliers\n",
    "        self._support_vectors=support_vectors\n",
    "        self._support_vector_labels=support_vector_labels\n",
    "\n",
    "\n",
    "    def _compute_multipliers(self, X, y):\n",
    "        \"\"\"\n",
    "        Given the data, label, solve the QP program to get lagrange multiplier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "\n",
    "        Returns\n",
    "        lagrange_multipliers: 1D array\n",
    "        -------\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "\n",
    "        K = self._kernel_matrix(X)\n",
    "        \"\"\"\n",
    "        The standard QP solver formulation:\n",
    "        min 1/2 alpha^T H alpha + f^T alpha\n",
    "        s.t.\n",
    "        A * alpha \\coneleq a (A is former G)\n",
    "        B * alpha = b\n",
    "        \"\"\"\n",
    "        # TODO: implement. Specifically, define the H, f, A, a, B, b arguments\n",
    "        # as indicated above.\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        B = cvxopt.matrix(y,(1,N))\n",
    "       \n",
    "        A = cvxopt.matrix((np.diag(np.ones(N))))\n",
    "        a = cvxopt.matrix((np.ones((N,1))*self.C))\n",
    "        \n",
    "        f = cvxopt.matrix(-1*np.ones([N]))\n",
    "        H = cvxopt.matrix(np.outer(y,y)*K)\n",
    "\n",
    "        solution = cvxopt.solvers.qp(H, f, A, a, B, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        return np.ravel(solution['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvFRZS1SghD1"
   },
   "source": [
    "#### A simple sentiment analysis on [tweets on US airline service quality](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). (WARNING: expletives unfiltered)\n",
    "---\n",
    "- As shown below, our data comes in the form of a csv table. The columns most relevant to our task are 'text' and 'airline_sentiment'.\n",
    "- Data must be represented as a [N x d] matrix, but what we have on our hands is unstructured text.\n",
    "- The simplest solution to transform an airline review into a vector is [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We maintain a global vocabulary of word patterns gathered from our corpus, with single words such as \"great\", \"horrible\", and optionally consecutive words (N-grams) like \"friendly service\", \"luggage lost\". Suppose we have already collected a total of 10000 such patterns, to transform a sentence into a 10000-dimensional vector, we simply scan it and look for the patterns that appear and set their correponding entries to 1 and leave the rest at 0. What we end up with is a sparse vector that can be fed into SVMs.\n",
    "- Our data is not balanced, with siginificant more negatives than neutral + positives. Therefore we group neutral and positive into one category and the final ratio of non-negative vs negative is about 1:2. This is consistent across train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUmn6ej2ghD2",
    "outputId": "253fe1c6-2074-4956-e6ba-0fb7e910fae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2145k  100 2145k    0     0  5195k      0 --:--:-- --:--:-- --:--:-- 5195k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  466k  100  466k    0     0  1713k      0 --:--:-- --:--:-- --:--:-- 1707k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  321k  100  321k    0     0  1290k      0 --:--:-- --:--:-- --:--:-- 1290k\n",
      "   Unnamed: 0            tweet_id  ...   tweet_location               user_timezone\n",
      "0        3816  568103618500530176  ...              NaN  Pacific Time (US & Canada)\n",
      "1       10306  569379979055796224  ...   Lewes, DE, USA  Eastern Time (US & Canada)\n",
      "2         292  568840724700995584  ...  Portland, Maine  Eastern Time (US & Canada)\n",
      "\n",
      "[3 rows x 16 columns]\n",
      "negative    5905\n",
      "neutral     1804\n",
      "positive    1442\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#@markdown Here, we'll set up the dataset. To show you that this is doing\n",
    "#@markdown something useful, we'll print the first three entries of the\n",
    "#@markdown training set.\n",
    "\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_root = 'data/tweets'\n",
    "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/train.csv\n",
    "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/val.csv\n",
    "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/test_release.csv\n",
    "\n",
    "data_root = ''\n",
    "train, val, test = \\\n",
    "    pd.read_csv(osp.join(data_root, 'train.csv')), \\\n",
    "    pd.read_csv(osp.join(data_root, 'val.csv')), \\\n",
    "    pd.read_csv(osp.join(data_root, 'test_release.csv'))\n",
    "\n",
    "print(train.head(3))\n",
    "\n",
    "print(train.airline_sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1Zp-mi8ghEB",
    "outputId": "fdf25d92-ec06-4bcf-d716-9468a0a24ed3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', 153), ('united', 155), ('airport', 4), ('http', 71), ('co', 29), ('hour', 69), ('hold', 65), ('please', 114), ('tell', 139), ('trying', 151)]\n",
      "\n",
      " vocabulary size 172\n"
     ]
    }
   ],
   "source": [
    "#@title Vectorizing the vocabulary\n",
    "#@markdown Here, we build the vocabulary and vector representations for each\n",
    "#@markdown word. You may find it useful to modify parts of this later, but it's\n",
    "#@markdown not strictly necessary. We then sample some of the learned\n",
    "#@markdown vocabulary.\n",
    "\n",
    "# check these out \n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_normalize(tweet):\n",
    "    only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    tokens = nltk.word_tokenize(only_letters)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas\n",
    "\n",
    "# the sklearn vectorizer scans our corpus, build the vocabulary, and changes text into vectors\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode', \n",
    "    lowercase=True, \n",
    "    tokenizer=tokenize_normalize,\n",
    "    ngram_range=(1,1),  # you may want to try 2 grams. The vocab will get very large though,\n",
    "    min_df=100,  # this parameter deletes words that occur in less than min_df\n",
    "                # documents. decreasing this will increase the vocabulary size,\n",
    "                # but may also increase the runtime.\n",
    ")\n",
    "# first learn the vocabulary\n",
    "vectorizer.fit(pd.concat([train, val]).text)\n",
    "\n",
    "\n",
    "print( list(vectorizer.vocabulary_.items())[:10] )\n",
    "print(\"\\n vocabulary size {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElkjTodLghEG",
    "outputId": "32238c8f-8650-4899-a3b7-787e7cf20792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9151, 172)\n"
     ]
    }
   ],
   "source": [
    "#@title Setting up the training set\n",
    "#@markdown Now, we will prepare the training data so that we can call our\n",
    "#@markdown SVM as a black-box.\n",
    "\n",
    "X = {}\n",
    "y = {}\n",
    "X['train'] = vectorizer.transform(train.text).toarray()\n",
    "X['val'] = vectorizer.transform(val.text).toarray()\n",
    "X['test'] = vectorizer.transform(test.text).toarray()\n",
    "\n",
    "# note that our data is 10250 dimensional. \n",
    "# This is a little daunting for laptops and coming up with a manageable vector\n",
    "# representation is a major topic in Natural Language Processing.\n",
    "print(X['train'].shape)\n",
    "\n",
    "# convert the word labels of 'positive', 'neutral', 'negative' into integer labels\n",
    "# note that positive and neural belong to one category, labelled as 1, while negative stands alone as the other\n",
    "for name, dataframe in zip(['train', 'val'], [train, val]):\n",
    "    sentiments_in_words = dataframe['airline_sentiment'].tolist()\n",
    "    int_lbls = np.array( list(map(lambda x: -1 if x == 'negative' else 1, sentiments_in_words)), dtype=np.int32 )\n",
    "    y[name] = int_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "536A07IYghEJ",
    "outputId": "c109efd2-167e-4f11-a3c6-af538dcae572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "6737 out of 9151 correct, acc 0.736\n",
      "1460 out of 2000 correct, acc 0.730\n"
     ]
    }
   ],
   "source": [
    "#@title Linear SVM on the airline dataset\n",
    "\n",
    "svm = LinearSVM(C=1000)\n",
    "svm.fit(X['train'], y['train'],lr_sched=lambda t: 1/(.1*t), num_epochs=10)\n",
    "compute_acc(svm, X['train'], y['train'])\n",
    "compute_acc(svm, X['val'], y['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1VkbuDr5ePe",
    "outputId": "f36f85c3-2385-454b-952d-d20eef69e6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9151, 172)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4259e+05 -4.4673e+07  3e+08  1e+00  2e+02\n",
      " 1: -3.7148e+05 -8.5264e+07  3e+08  1e+00  2e+02\n",
      " 2:  1.5827e+05 -1.1333e+08  2e+08  6e-01  1e+02\n",
      " 3:  4.4231e+05 -7.8572e+07  1e+08  3e-01  5e+01\n",
      " 4:  6.6556e+05 -6.4774e+07  7e+07  2e-01  3e+01\n",
      " 5:  9.1816e+05 -4.8498e+07  5e+07  1e-01  2e+01\n",
      " 6:  9.4303e+05 -1.2145e+06  2e+06  4e-11  8e-12\n",
      " 7:  6.0202e+05 -9.6034e+05  2e+06  3e-11  8e-12\n",
      " 8:  9.9535e+04 -7.7673e+05  9e+05  1e-11  6e-12\n",
      " 9:  4.2529e+04 -7.9405e+05  8e+05  3e-11  7e-12\n",
      "10: -1.9516e+05 -7.5152e+05  6e+05  3e-11  7e-12\n",
      "11: -3.6036e+05 -7.3163e+05  4e+05  1e-11  8e-12\n",
      "12: -5.3453e+05 -6.6111e+05  1e+05  2e-11  1e-11\n",
      "13: -5.5868e+05 -6.3744e+05  8e+04  1e-11  1e-11\n",
      "14: -5.8218e+05 -6.1820e+05  4e+04  2e-12  9e-12\n",
      "15: -5.8854e+05 -6.1304e+05  2e+04  3e-13  1e-11\n",
      "16: -5.9500e+05 -6.0907e+05  1e+04  6e-12  1e-11\n",
      "17: -6.0053e+05 -6.0257e+05  2e+03  5e-11  1e-11\n",
      "18: -6.0146e+05 -6.0148e+05  3e+01  9e-12  1e-11\n",
      "19: -6.0147e+05 -6.0147e+05  3e-01  2e-11  1e-11\n",
      "Optimal solution found.\n",
      "SV number:  8785\n",
      "5674 out of 9151 correct, acc 0.620\n",
      "1223 out of 2000 correct, acc 0.612\n"
     ]
    }
   ],
   "source": [
    "#@title Kernel SVM with linear kernel on the airline dataset (optional)\n",
    "#@markdown This could take a while to run.\n",
    "\n",
    "print(X['train'].shape)\n",
    "\n",
    "svm = KernelSVM(Kernel.linear(), C=100)\n",
    "svm.fit(X['train'].astype(float), y['train'].astype(float))\n",
    "compute_acc(svm, X['train'], y['train'])\n",
    "compute_acc(svm, X['val'], y['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2u8o5Y5HAr6",
    "outputId": "995a8674-1f65-455a-9597-474e778995eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9151, 172)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6081e+04 -2.8677e+05  2e+05  2e-12  9e-12\n",
      " 1: -5.4265e+04 -7.3998e+04  2e+04  4e-13  8e-12\n",
      " 2: -6.4813e+04 -6.5560e+04  7e+02  4e-13  2e-12\n",
      " 3: -6.4919e+04 -6.4926e+04  7e+00  1e-12  9e-12\n",
      " 4: -6.4920e+04 -6.4920e+04  7e-02  1e-13  1e-11\n",
      " 5: -6.4920e+04 -6.4920e+04  7e-04  2e-12  8e-12\n",
      "Optimal solution found.\n",
      "SV number:  9151\n",
      "4832 out of 9151 correct, acc 0.528\n",
      "861 out of 2000 correct, acc 0.430\n"
     ]
    }
   ],
   "source": [
    "#@title Kernel SVM with RBF kernel on the airline dataset (optional)\n",
    "#@markdown This could take a while to run.\n",
    "\n",
    "print(X['train'].shape)\n",
    "\n",
    "svm = KernelSVM(Kernel.gaussian(sigma=1), C=10)\n",
    "svm.fit(X['train'].astype(float), y['train'].astype(float))\n",
    "compute_acc(svm, X['train'], y['train'])\n",
    "compute_acc(svm, X['val'], y['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e9OUfEQghEO"
   },
   "source": [
    "- Try to come up with a better text feature representation. We threw out all the emojis. >< what a waste\n",
    "- Play around with different parameter settings and upload your test set predictions to kaggle.\n",
    "- Given the high feature dimensionality of our primitive text processing, we do not recommend using kernel SVM here. It could take a long time to train. If you reduce the feature dimensionality, then it's a different story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "QxC9ZhkIghER",
    "outputId": "bf6a0071-dd8c-468d-e4b0-afd48a943201"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3d5192f6-2d4e-463c-833a-9f62dc114343\", \"tweet_test.csv\", 30383)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Generate test output\n",
    "#@markdown Run this cell to generate the csv containing your test outputs for\n",
    "#@markdown Kaggle.\n",
    "#@markdown Contest link: https://www.kaggle.com/c/ttic31020hw3\n",
    "\n",
    "test_pred = svm.predict(X['test'])\n",
    "create_download_file('tweet_test.csv', test_pred)\n",
    "from google.colab import files\n",
    "files.download('tweet_test.csv') "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of ps3_student_copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
